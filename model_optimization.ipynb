{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models import naive_bayes, logistic, random_forest\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "data = pd.read_csv('data/clean_data.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results from the initial model fitting, it seems as though the numerical features we collected using Tweepy were not particularly useful. This could be because the features themselves are not informative or because the imputation techniques we used for the missing tweets were not good. If we have time, we can go back and try more advanced imputation techniques. For now, let's ignore the numerical features except for date_time. Date_time should be reliable because tweets are organized chronologically by tweet id and so the missing date_times we imputed should be fairly reliable. Let's also keep numerical features that we collected directly from the text like exclamation_mark_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>is_quoted</th>\n",
       "      <th>date_time</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>followers</th>\n",
       "      <th>verified</th>\n",
       "      <th>location</th>\n",
       "      <th>exclamation_mark_count</th>\n",
       "      <th>question_mark_count</th>\n",
       "      <th>imputed</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaaaand delet glob warm rain tweet cas miss s...</td>\n",
       "      <td>794050846807982080</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-11-03 05:37:29</td>\n",
       "      <td>47</td>\n",
       "      <td>79</td>\n",
       "      <td>20106</td>\n",
       "      <td>1</td>\n",
       "      <td>NYC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaaaand go trump admin start remov clim chang ...</td>\n",
       "      <td>828858786286796800</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-07 07:07:01</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5164</td>\n",
       "      <td>0</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaaand elimin ref glob warm wisconsin</td>\n",
       "      <td>814547316258512896</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-12-29 18:58:31</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5164</td>\n",
       "      <td>0</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aaaand ep remov clim chang pag stil check ever...</td>\n",
       "      <td>858153629638959106</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-29 02:59:03</td>\n",
       "      <td>760</td>\n",
       "      <td>637</td>\n",
       "      <td>428254</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ab act clim chang cal show success vis amp pract</td>\n",
       "      <td>890334462004699136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-26 22:14:12</td>\n",
       "      <td>38</td>\n",
       "      <td>76</td>\n",
       "      <td>19398</td>\n",
       "      <td>1</td>\n",
       "      <td>Lakewood, CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean text             tweetid  \\\n",
       "2  aaaaaand delet glob warm rain tweet cas miss s...  794050846807982080   \n",
       "3  aaaaand go trump admin start remov clim chang ...  828858786286796800   \n",
       "4              aaaand elimin ref glob warm wisconsin  814547316258512896   \n",
       "5  aaaand ep remov clim chang pag stil check ever...  858153629638959106   \n",
       "6   ab act clim chang cal show success vis amp pract  890334462004699136   \n",
       "\n",
       "   is_retweet  is_quoted           date_time  retweets  favorites  followers  \\\n",
       "2           1          0 2016-11-03 05:37:29        47         79      20106   \n",
       "3           1          0 2017-02-07 07:07:01         2          3       5164   \n",
       "4           1          0 2016-12-29 18:58:31         2          3       5164   \n",
       "5           1          0 2017-04-29 02:59:03       760        637     428254   \n",
       "6           1          0 2017-07-26 22:14:12        38         76      19398   \n",
       "\n",
       "   verified      location  exclamation_mark_count  question_mark_count  \\\n",
       "2         1           NYC                       0                    0   \n",
       "3         0             n                       0                    0   \n",
       "4         0             n                       0                    0   \n",
       "5         0           NaN                       0                    0   \n",
       "6         1  Lakewood, CA                       0                    0   \n",
       "\n",
       "   imputed  sentiment  month  hour  dayofweek  \n",
       "2    False         -1     11     5          3  \n",
       "3     True          1      2     7          1  \n",
       "4     True          1     12    18          3  \n",
       "5    False          1      4     2          5  \n",
       "6    False          1      7    22          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding date time features\n",
    "data['date_time'] = pd.to_datetime(data['date_time'])\n",
    "\n",
    "# might not be a good idea to use year as a feature, since we want to be able to make predictions in \n",
    "# future years not contained in training data\n",
    "# let's use hour and month since these correlate with temperature\n",
    "data['month'] = data['date_time'].dt.month\n",
    "data['hour'] = data['date_time'].dt.hour\n",
    "# lets also include day of week since that can influence sentiment\n",
    "data['dayofweek'] = data['date_time'].dt.dayofweek\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 1:  0.6481264232972189\n",
      "class 0:  0.22859015940928853\n",
      "class -1:  0.12328341729349251\n"
     ]
    }
   ],
   "source": [
    "# upsampling seemed to improve results so let's continue doing this\n",
    "\n",
    "n = len(data)\n",
    "print('class 1: ', len(data[data['sentiment']==1])/n)\n",
    "print('class 0: ', len(data[data['sentiment']==0])/n)\n",
    "print('class -1: ', len(data[data['sentiment']==-1])/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 1:  0.3333333333333333\n",
      "class 0:  0.3333333333333333\n",
      "class -1:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "diffneg = (len(data[data['sentiment']==1]) - \n",
    "        len(data[data['sentiment']==-1])) # want to creat 33-33-33 balance\n",
    "diffneu = (len(data[data['sentiment']==1]) - \n",
    "        len(data[data['sentiment']==0])) # want to creat 33-33-33 balance\n",
    "\n",
    "neg_data = data[data['sentiment']==-1] # sample to choose from\n",
    "neu_data = data[data['sentiment']==0] # sample to choose from\n",
    "\n",
    "upsample_index_neg = np.random.choice(neg_data.index,size=diffneg) # sample with repetition\n",
    "upsample_index_neu = np.random.choice(neu_data.index,size=diffneu) # sample with repetition\n",
    "\n",
    "data_ups = data.append(neg_data.loc[upsample_index_neg]).append(neu_data.loc[upsample_index_neu]).sample(frac=1)\n",
    "\n",
    "n = len(data_ups)\n",
    "print('class 1: ', len(data_ups[data_ups['sentiment']==1])/n)\n",
    "print('class 0: ', len(data_ups[data_ups['sentiment']==0])/n)\n",
    "print('class -1: ', len(data_ups[data_ups['sentiment']==-1])/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into testing and training sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(data_ups.drop(labels = \"sentiment\", axis = 1), \n",
    "                                                    data_ups[\"sentiment\"], test_size=0.2, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select just the numerical data\n",
    "numerical_feature_names = ['exclamation_mark_count', 'question_mark_count','month','hour','dayofweek']\n",
    "X_train_numerical = X_train.loc[:,numerical_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "n-gram:  (1, 1)\n",
      "Accuracy:  0.8962756265671498\n",
      "Precision:  0.8971316026935282\n",
      "Recall:  0.8963747930448666\n",
      "F score:  0.8952602867572047\n",
      "n-gram:  (1, 2)\n",
      "Accuracy:  0.8921496923045584\n",
      "Precision:  0.89496521274199\n",
      "Recall:  0.8922580278359604\n",
      "F score:  0.8911518410132888\n",
      "n-gram:  (1, 3)\n",
      "Accuracy:  0.8909296981084432\n",
      "Precision:  0.8945634687950432\n",
      "Recall:  0.891035682712479\n",
      "F score:  0.8899493985250444\n",
      "n-gram:  (2, 2)\n",
      "Accuracy:  0.8900424038828756\n",
      "Precision:  0.8918710181642598\n",
      "Recall:  0.8901701293288931\n",
      "F score:  0.8897688771811273\n",
      "n-gram:  (2, 3)\n",
      "Accuracy:  0.8864266983029886\n",
      "Precision:  0.8883014053458954\n",
      "Recall:  0.8865457950462232\n",
      "F score:  0.8858787897968792\n",
      "n-gram:  (3, 3)\n",
      "Accuracy:  0.8554601561538252\n",
      "Precision:  0.8624233906219041\n",
      "Recall:  0.8556111031621623\n",
      "F score:  0.8528549248367756\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n",
      "n-gram:  (1, 1)\n",
      "Accuracy:  0.9022648853457224\n",
      "Precision:  0.9021834624408583\n",
      "Recall:  0.9023819409498209\n",
      "F score:  0.901568657069253\n",
      "n-gram:  (1, 2)\n",
      "Accuracy:  0.8868259761532957\n",
      "Precision:  0.8907798232489604\n",
      "Recall:  0.8869554893074284\n",
      "F score:  0.885568158192803\n",
      "n-gram:  (1, 3)\n",
      "Accuracy:  0.8625806829040954\n",
      "Precision:  0.8756078211674257\n",
      "Recall:  0.8627440002345343\n",
      "F score:  0.8594468980462784\n",
      "n-gram:  (2, 2)\n",
      "Accuracy:  0.8415962641403268\n",
      "Precision:  0.8612869476301219\n",
      "Recall:  0.8417924553797971\n",
      "F score:  0.8370130968946821\n",
      "n-gram:  (2, 3)\n",
      "Accuracy:  0.8167077192946891\n",
      "Precision:  0.8487122153493386\n",
      "Recall:  0.8169246595241126\n",
      "F score:  0.8072335881831705\n",
      "n-gram:  (3, 3)\n",
      "Accuracy:  0.8458552754390087\n",
      "Precision:  0.8538941374681341\n",
      "Recall:  0.8460664414031621\n",
      "F score:  0.8420764979400426\n"
     ]
    }
   ],
   "source": [
    "# compare 5 fold cross validation error metrics for count and tfidf vectorizer with\n",
    "# 1, 2, 3-grams\n",
    "\n",
    "for skvectorizeri in [CountVectorizer,TfidfVectorizer]:\n",
    "    print(skvectorizeri)\n",
    "    for ngrami in [(1,1),(1,2),(1,3),(2,2),(2,3),(3,3)]:\n",
    "        rf = random_forest(X_train[\"clean text\"].values, y_train.values, X_train_numerical.values,\n",
    "                           ngram=ngrami,skvectorizer=skvectorizeri)\n",
    "        print('n-gram: ',ngrami)\n",
    "        print('Accuracy: ',rf[0])\n",
    "        print('Precision: ',rf[1])\n",
    "        print('Recall: ',rf[2])\n",
    "        print('F score: ',rf[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF vectorizer with 1-grams had the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests perform implicit feature selection by splitting on the most important nodes, but it can still be useful to look at which features it deemed the most important to speed up the model and potentially use as feature selection for other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on all training data\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "features = vectorizer.fit_transform(X_train[\"clean text\"].values)\n",
    "features = hstack([features, csr_matrix(X_train_numerical.values)])\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(features, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names() + numerical_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: chang  |  0.02154\n",
      "feature: clim  |  0.02121\n",
      "feature: hour  |  0.01717\n",
      "feature: glob  |  0.01531\n",
      "feature: month  |  0.01508\n",
      "feature: warm  |  0.01427\n",
      "feature: dayofweek  |  0.01354\n",
      "feature: deny  |  0.00774\n",
      "feature: lib  |  0.00699\n",
      "feature: sci  |  0.0064\n",
      "feature: scam  |  0.0057\n",
      "feature: real  |  0.00495\n",
      "feature: trump  |  0.00479\n",
      "feature: obam  |  0.00472\n",
      "feature: exclamation_mark_count  |  0.00457\n",
      "feature: believ  |  0.00419\n",
      "feature: fight  |  0.0041\n",
      "feature: mad  |  0.0039\n",
      "feature: man  |  0.00383\n",
      "feature: alarm  |  0.00364\n",
      "feature: question_mark_count  |  0.00357\n",
      "feature: hoax  |  0.00353\n",
      "feature: fak  |  0.0034\n",
      "feature: caus  |  0.00338\n",
      "feature: us  |  0.0033\n",
      "feature: left  |  0.00323\n",
      "feature: say  |  0.00318\n",
      "feature: act  |  0.00317\n",
      "feature: amp  |  0.00308\n",
      "feature: think  |  0.00301\n",
      "feature: new  |  0.00294\n",
      "feature: peopl  |  0.00293\n",
      "feature: blam  |  0.00291\n",
      "feature: lik  |  0.00273\n",
      "feature: tax  |  0.00271\n",
      "feature: dont  |  0.00268\n",
      "feature: year  |  0.00265\n",
      "feature: on  |  0.00258\n",
      "feature: cal  |  0.00257\n",
      "feature: is  |  0.00243\n",
      "feature: via  |  0.00238\n",
      "feature: world  |  0.00236\n",
      "feature: hum  |  0.00226\n",
      "feature: nee  |  0.00226\n",
      "feature: gor  |  0.00223\n",
      "feature: know  |  0.00222\n",
      "feature: https  |  0.00217\n",
      "feature: weath  |  0.00215\n",
      "feature: den  |  0.00213\n",
      "feature: stop  |  0.0021\n",
      "feature: talk  |  0.00209\n",
      "feature: mak  |  0.00204\n",
      "feature: presid  |  0.00202\n",
      "feature: doesnt  |  0.00201\n",
      "feature: im  |  0.00199\n",
      "feature: ev  |  0.00196\n",
      "feature: right  |  0.00196\n",
      "feature: get  |  0.00194\n",
      "feature: thing  |  0.00194\n",
      "feature: tcot  |  0.00193\n",
      "feature: lead  |  0.00192\n",
      "feature: lie  |  0.00191\n",
      "feature: nat  |  0.00189\n",
      "feature: relig  |  0.00185\n",
      "feature: said  |  0.00185\n",
      "feature: dat  |  0.0018\n",
      "feature: tim  |  0.0018\n",
      "feature: would  |  0.00177\n",
      "feature: see  |  0.00174\n",
      "feature: money  |  0.00172\n",
      "feature: tak  |  0.00172\n",
      "feature: stil  |  0.0017\n",
      "feature: prov  |  0.00169\n",
      "feature: good  |  0.00163\n",
      "feature: solv  |  0.00161\n",
      "feature: isnt  |  0.00159\n",
      "feature: want  |  0.00158\n",
      "feature: ter  |  0.00157\n",
      "feature: com  |  0.00153\n",
      "feature: al  |  0.0015\n",
      "feature: cant  |  0.0015\n",
      "feature: environ  |  0.0015\n",
      "feature: could  |  0.00147\n",
      "feature: effect  |  0.00146\n",
      "feature: much  |  0.00145\n",
      "feature: hur  |  0.00144\n",
      "feature: hot  |  0.00143\n",
      "feature: planet  |  0.00143\n",
      "feature: polit  |  0.00141\n",
      "feature: threat  |  0.00141\n",
      "feature: bil  |  0.0014\n",
      "feature: going  |  0.0014\n",
      "feature: cold  |  0.00139\n",
      "feature: ic  |  0.00139\n",
      "feature: impact  |  0.00139\n",
      "feature: climatechang  |  0.00138\n",
      "feature: ear  |  0.00138\n",
      "feature: fraud  |  0.00138\n",
      "feature: co  |  0.00136\n",
      "feature: hap  |  0.00136\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/improving-random-forest-in-python-part-1-893916666cd\n",
    "\n",
    "importances = list(rf_classifier.feature_importances_) # higher value = more important\n",
    "feature_importances = [(feature, round(importance, 5)) for feature, importance in zip(feature_names, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "for i in range(100): # print top 100 most important features\n",
    "    print('feature:', feature_importances[i][0],' | ',feature_importances[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these features make sense! There are many words related to climate change (chang, clim, warm, glob), belief (deny, scam, real, believ, fak, hoax), and politcs (lib, obam, trump). It's also intersting to note that all our numerical features were pretty important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
