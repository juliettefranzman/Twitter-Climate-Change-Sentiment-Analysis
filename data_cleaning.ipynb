{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tweepy\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "#nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lstem = LancasterStemmer()\n",
    "\n",
    "import emoji\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold # import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load kaggle dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/edqian/twitter-climate-change-sentiment-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43943\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@tiniebeany climate change is an interesting h...</td>\n",
       "      <td>792927353886371840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n",
       "      <td>793124211518832641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n",
       "      <td>793124402388832256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @Mick_Fanning: Just watched this amazing do...</td>\n",
       "      <td>793124635873275904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @cnalive: Pranita Biswasi, a Lutheran from ...</td>\n",
       "      <td>793125156185137153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  @tiniebeany climate change is an interesting h...   \n",
       "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
       "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
       "3          1  RT @Mick_Fanning: Just watched this amazing do...   \n",
       "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
       "\n",
       "              tweetid  \n",
       "0  792927353886371840  \n",
       "1  793124211518832641  \n",
       "2  793124402388832256  \n",
       "3  793124635873275904  \n",
       "4  793125156185137153  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data = pd.read_csv(\"twitter_sentiment_data.csv\")\n",
    "n = len(tweet_data)\n",
    "\n",
    "print(n)\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get additional features and uncorrupted text using tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the tweets in the orginal dataset appear to have corrupted and unreadable text, so we decided to use the tweet id's and Tweepy to find all the original tweets as well as the following additional featues: <br>\n",
    " - retweet (T/F)\n",
    " - quoted (T/F)\n",
    " - quoted text\n",
    " - date/time\n",
    " - retweet count\n",
    " - favorite count\n",
    " - hashtags\n",
    " - follower count\n",
    " - verified (T/F)\n",
    " - location\n",
    " - geographic coordinates\n",
    " - language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own credentials here\n",
    "twitter_keys = {'consumer_key':        '',\n",
    "                'consumer_secret':     '',\n",
    "                'access_token_key':    '',\n",
    "                'access_token_secret': ''}\n",
    "\n",
    "# Setup access to API\n",
    "auth = tweepy.OAuthHandler(twitter_keys['consumer_key'], twitter_keys['consumer_secret'])\n",
    "auth.set_access_token(twitter_keys['access_token_key'], twitter_keys['access_token_secret'])\n",
    "\n",
    "api = tweepy.API(auth,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweepy's api.get_status is limited to 900 calls every 15 minutes\n",
    "# so we wrote our code to pause before hitting the rate limit\n",
    "\n",
    "text = ['n']*n # n is size of dataframe\n",
    "is_retweet = ['n']*n\n",
    "is_quoted = ['n']*n\n",
    "quoted_text = ['n']*n\n",
    "date_time = ['n']*n\n",
    "retweets = [-999]*n\n",
    "favorites = [-999]*n\n",
    "hashtags = ['n']*n\n",
    "followers = [-999]*n\n",
    "verified = [-999]*n\n",
    "location = ['n']*n\n",
    "coordinate = [-999]*n\n",
    "language = ['n']*n\n",
    "\n",
    "stop_here = np.arange(850,n,850) # need to stop every 900 calls, stopping every 850 to be safe\n",
    "\n",
    "for i in range(n): \n",
    "    if i in stop_here: # stop before hitting rate limit\n",
    "        time.sleep(15*60) # sleep for 15 minutes\n",
    "        print('Sleeping!')\n",
    "        \n",
    "    try:\n",
    "        tweet = api.get_status(tweet_data['tweetid'].iloc[i]) # create status object, this is 1 API call\n",
    "        \n",
    "        if hasattr(tweet, 'retweeted_status'): # retweet\n",
    "            text[i] = tweet.text # tweet body\n",
    "            is_retweet[i] = True # is a retweet\n",
    "            is_quoted[i] = False # not a quote\n",
    "            quoted_text[i] = '' # not a quote\n",
    "            date_time[i] = tweet.created_at # time created\n",
    "            retweets[i] = tweet.retweeted_status.retweet_count # retweet count\n",
    "            favorites[i] = tweet.retweeted_status.favorite_count # favorite count\n",
    "            hashtags[i] = tweet.retweeted_status.entities['hashtags'] # hashtags\n",
    "            followers[i] = tweet.retweeted_status.user.followers_count # follower count\n",
    "            verified[i] = tweet.retweeted_status.user.verified # verified\n",
    "            location[i] = tweet.retweeted_status.user.location # location\n",
    "            coordinate[i] = tweet.retweeted_status.coordinates # coordinates\n",
    "            language[i] = tweet.lang # language  ##edited from tweet.retweeted_status.user.lang\n",
    "            \n",
    "        else: \n",
    "            text[i] = tweet.text # tweet body\n",
    "            is_retweet[i] = False # not a retweet\n",
    "            date_time[i] = tweet.created_at # time created\n",
    "            retweets[i] = tweet.retweet_count # retweet count\n",
    "            favorites[i] = tweet.favorite_count # favorite count\n",
    "            hashtags[i] = tweet.entities['hashtags'] # hashtags\n",
    "            followers[i] = tweet.user.followers_count # follower count\n",
    "            verified[i] = tweet.user.verified # verified\n",
    "            location[i] = tweet.user.location # location\n",
    "            coordinate[i] = tweet.coordinates # coordinates\n",
    "            language[i] = tweet.lang # language\n",
    "            \n",
    "            if tweet.is_quote_status: # quoted\n",
    "                is_quoted[i] = True # is a quote\n",
    "                quoted_text[i] = tweet.quoted_status.text #is a quote\n",
    "            else:\n",
    "                is_quoted[i] = False # not a quote\n",
    "                quoted_text[i] = '' # not a quote \n",
    "    \n",
    "    # some tweets are no longer available, skip \n",
    "    except tweepy.TweepError:\n",
    "        pass \n",
    "    \n",
    "    # if tweet.is_quote_status = True, but the quoted tweet has been deleted, skip\n",
    "    except AttributeError:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment the original dataframe with additional features\n",
    "\n",
    "tweet_data['text'] = text\n",
    "tweet_data['is_retweet'] = is_retweet\n",
    "tweet_data['is_quoted'] = is_quoted\n",
    "tweet_data['quoted_text'] = quoted_text\n",
    "tweet_data['date_time'] = date_time\n",
    "tweet_data['retweets'] = retweets\n",
    "tweet_data['favorites'] = favorites\n",
    "tweet_data['hashtags'] = hashtags\n",
    "tweet_data['followers'] = followers\n",
    "tweet_data['verified'] = verified\n",
    "tweet_data['location'] = location\n",
    "tweet_data['coordinate'] = coordinate\n",
    "tweet_data['language'] = language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load augmented kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21972\n",
      "21971\n",
      "43943\n",
      "Proportion missing tweets:  0.29\n"
     ]
    }
   ],
   "source": [
    "# the above code takes a while to run\n",
    "# we each ran the code on one half of the data and saved the results\n",
    "# here are the two halves combined into one dataset\n",
    "\n",
    "aug1 = pd.read_csv('augmented_data1.csv')\n",
    "aug2 = pd.read_csv('augmented_data2.csv')\n",
    "aug_data = aug1.append(aug2)\n",
    "\n",
    "print(len(aug1))\n",
    "print(len(aug2))\n",
    "print(len(aug_data))\n",
    "print('Proportion missing tweets: ',round(len(aug_data[aug_data['text']=='n'])/len(aug_data),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 29% of the tweets were no longer accessible (deleted, account on private, reported, etc.), so we had to decide how to clean the original corrupted text and how to impute the additional features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date/time\n",
    "For date/time we found that the tweet id's were ordered by date, so we could impute by choosing the date/time inbetween the ones we did have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 'n' with proper null value to prevent error in time conversion\n",
    "# change date_time from string to pandas timestamp\n",
    "\n",
    "aug_data['date_time'] = pd.to_datetime(aug_data['date_time'].replace('n',np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting by tweetid appears to sort by date time\n",
    "\n",
    "aug_data = aug_data.sort_values('tweetid').drop(['Unnamed: 0'],axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find inbetween date/times\n",
    "\n",
    "times = aug_data['date_time']\n",
    "imputed_times = []\n",
    "for i in range(len(times)): \n",
    "    if pd.isnull(times.iloc[i]): # if the time is null (NaT)\n",
    "        if pd.isnull(times.iloc[i-1]): # if the previous time was also null\n",
    "            imputed_times.append(imputed_times[-1]) # just use what we imputed for previous\n",
    "        else: # if the previous time was not null\n",
    "            time1 = times.iloc[i-1] # the previous time\n",
    "            time2_i = times.iloc[i:].first_valid_index() # the index of the next non null time\n",
    "            if time2_i == None: # if the remaining times are all null\n",
    "                imputed_times.append(imputed_times[-1]) # use the previous time\n",
    "            else:\n",
    "                time2 = times.iloc[time2_i] # the next non null time\n",
    "                diff = pd.Timedelta(time2 - time1).seconds # difference in times\n",
    "                imputed_times.append(time1 + pd.to_timedelta(diff/2, unit='s')) # add half the time\n",
    "    else: # if the time is not null just use what we have\n",
    "        imputed_times.append(times.iloc[i])\n",
    "\n",
    "aug_data['date_time'] = imputed_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropping data\n",
    "We removed all duplicate tweets as well as languages not identified as English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some entries appear to be duplicates (retweets of the same tweet)\n",
    "# we do not really need these, the only thing that differs between them is the time of the\n",
    "# retweet, so we kept the earliest time and removed duplicates\n",
    "\n",
    "aug_data = aug_data.groupby(\"message\", as_index=False).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove languages that we know are not english\n",
    "\n",
    "aug_data = aug_data[(aug_data['language']=='en') | (aug_data['language']=='n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boolean features\n",
    "If a tweet started with \"RT\" we filled is_retweet = True. We found that most of the tweets were not quoted, so we filled the missing ones as is_quoted = False. Finally, we assumed that verified Twitter users carried more bias, so we filled in the missing values as is_verified = False. We additionally mapped True = 1 and False = 0 to prepare for model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert a column of string True False and bool True False into all bool\n",
    "\n",
    "def str_to_bool(x):\n",
    "    if type(x)==bool:\n",
    "        return x\n",
    "    else:\n",
    "        if x == 'True':\n",
    "            return True\n",
    "        if x == 'False':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing values for is_retweet\n",
    "\n",
    "for i in aug_data.index:\n",
    "    if aug_data['is_retweet'].loc[i] == 'n':\n",
    "        tweet = aug_data['message'].loc[i]\n",
    "        aug_data.at[i,'is_retweet'] = bool(re.search(r'^RT', tweet))\n",
    "\n",
    "# convert boolean to 0 and 1\n",
    "aug_data['is_retweet'] = aug_data['is_retweet'].apply(str_to_bool)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing values for is_quoted\n",
    "\n",
    "for i in aug_data.index:\n",
    "    if aug_data['is_quoted'].loc[i] == 'n':\n",
    "        aug_data.at[i,'is_quoted'] = False\n",
    "        \n",
    "# convert boolean to 0 and 1\n",
    "aug_data['is_quoted'] = aug_data['is_quoted'].apply(str_to_bool)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing values for is_verified\n",
    "\n",
    "for i in aug_data.index:\n",
    "    if aug_data['verified'].loc[i] == '-999':\n",
    "        aug_data.at[i,'verified'] = False\n",
    "        \n",
    "# convert boolean to 0 and 1\n",
    "aug_data['verified'] = aug_data['verified'].apply(str_to_bool)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did several things to clean the tweet text:\n",
    " - expand acronyms\n",
    " - convert emoticons to the textual emotion they convey (ex. :-) to happy)\n",
    " - convert emojis to the text\n",
    " - remove username, RT, and url\n",
    " - lowercase\n",
    " - count number of question and exclamation marks\n",
    " - remove punctuation\n",
    " - remove stop words\n",
    " - stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries of common emoticons and acronyms\n",
    "\n",
    "special_df = pd.read_csv(\"special.csv\")\n",
    "acronym_df = pd.read_csv(\"acronyms.csv\")\n",
    "\n",
    "special_dict = pd.Series(special_df['0'].values,index=special_df['1']).to_dict()\n",
    "acronym_dict = pd.Series(acronym_df['0'].values,index=acronym_df['1']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the tweet message\n",
    "# not sure if separating hashtags out is beneficial so leaving them in as normal words for now\n",
    "\n",
    "tokens = []\n",
    "question_marks = []\n",
    "exclamation_marks = []\n",
    "for i in range(len(aug_data)):\n",
    "    if aug_data['text'].iloc[i] == 'n': # if we were unable to retrieve the original tweet\n",
    "        txt = aug_data['message'].iloc[i] # use the original dataset's text\n",
    "    else:\n",
    "        if aug_data['is_quoted'].iloc[i] == True: # if it's a quoted tweet\n",
    "            txt = aug_data['text'].iloc[i] # only using comment (not quoted text)\n",
    "        else:\n",
    "            txt = aug_data['text'].iloc[i]\n",
    "        \n",
    "    ct = re.sub(r'@[A-Z0-9a-z_:]+','',txt) # remove username\n",
    "    ct = re.sub(r'^RT','',ct) # remove RT\n",
    "    ct = re.sub('https?://[A-Za-z0-9./]+','',ct) # remove urls\n",
    "    ct = emoji.demojize(ct) # change emojis into text\n",
    "    ct = ct.lower() # make lowercase\n",
    "    \n",
    "    for key,value in special_dict.items(): # go through all special characters (emoticons etc)\n",
    "        ct = re.sub(r'%s'%re.escape(key),value,ct) # if tweet contains special character replace it\n",
    "    for key,value in acronym_dict.items():\n",
    "        ct = re.sub(r'\\b%s\\b'%key,value,ct) \n",
    "        \n",
    "    q_mark = ct.count('?') # number of question marks\n",
    "    e_mark = ct.count('!') # number of exclamation marks\n",
    "    ct = re.sub(\"[']\", \"\",ct) # contractions remove '\n",
    "    ct = re.sub(\"[^a-zA-Z_]\", \" \",ct) # remove punctuation\n",
    "    \n",
    "    token = re.findall(r'\\S+', ct) # tokenize\n",
    "    token = [lstem.stem(word) for word in token if word not in stop_words] # stemming and removing stop words\n",
    "    \n",
    "    tokens.append(token)\n",
    "    question_marks.append(q_mark)\n",
    "    exclamation_marks.append(e_mark)\n",
    "    \n",
    "aug_data['tokens'] = tokens\n",
    "aug_data['exclamation_mark_count'] = question_marks\n",
    "aug_data['question_mark_count'] = exclamation_marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We imputed missing numerical values with the median rather than the mean since there were many extreme values. This is a pretty big simplification, so if it seems as though these features give poor performace we will try other imputation methods such as KNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing retweet, favorite, and follower counts\n",
    "\n",
    "retweets_med = np.median(aug_data[aug_data['retweets']!=-999]['retweets'])\n",
    "aug_data['retweets'] = aug_data['retweets'].replace(-999,retweets_med)\n",
    "\n",
    "favorites_med = np.median(aug_data[aug_data['favorites']!=-999]['favorites'])\n",
    "aug_data['favorites'] = aug_data['favorites'].replace(-999,favorites_med)\n",
    "\n",
    "followers_med = np.median(aug_data[aug_data['followers']!=-999]['followers'])\n",
    "aug_data['followers'] = aug_data['followers'].replace(-999,followers_med)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropping data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed empty strings after cleaning as well as duplicates. We also decided to drop class 2 from our analysis. Based on our own judgement, it was ambiguous as to whether many of the class 2 tweets really linked to factual news about climate change. Furthermore, we removed the URL's in the tweets which largley reduced the meaning of this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tokens that are empty lists\n",
    "\n",
    "aug_data = aug_data[aug_data['tokens'].map(lambda d: len(d)) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating clean text column\n",
    "\n",
    "aug_data['clean text'] = aug_data['tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicates again after cleaning\n",
    "\n",
    "aug_data = aug_data.sort_values('date_time').groupby('clean text',as_index=False).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing category 2 fromn our analysis\n",
    "\n",
    "aug_data = aug_data[aug_data['sentiment']!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to specify whether features were imputed or not\n",
    "\n",
    "def imputed(x):\n",
    "    if x == 'n':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column for imputed feature\n",
    "\n",
    "aug_data['imputed'] = aug_data['text'].apply(imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting issue with this row so dropping\n",
    "\n",
    "aug_data = aug_data.drop(12652)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = aug_data[['clean text',\n",
    "                       'tweetid',\n",
    "                       'is_retweet',\n",
    "                       'is_quoted',\n",
    "                       'date_time',\n",
    "                       'retweets',\n",
    "                       'favorites',\n",
    "                       'followers',\n",
    "                       'verified',\n",
    "                       'location',\n",
    "                       'exclamation_mark_count', \n",
    "                       'question_mark_count',\n",
    "                       'imputed',\n",
    "                       'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.astype({\"tweetid\": str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean text                        object\n",
       "tweetid                           object\n",
       "is_retweet                         int64\n",
       "is_quoted                          int64\n",
       "date_time                 datetime64[ns]\n",
       "retweets                           int64\n",
       "favorites                          int64\n",
       "followers                          int64\n",
       "verified                           int64\n",
       "location                          object\n",
       "exclamation_mark_count             int64\n",
       "question_mark_count                int64\n",
       "imputed                             bool\n",
       "sentiment                          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# clean_data.to_csv('clean_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
